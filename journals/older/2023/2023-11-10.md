- benchmarking #Polars #Parquet
- ### Generate random data
  1_000_000_000 records = 31 jaar seconde resolutie
  ```python
  import polars as pl
  import numpy as np
  import pandas as pd
  def ppl(start, end, n):
     start_u = start.value//10**9
     end_u = end.value//10**9
  return pl.DataFrame((10**9*np.random.randint(start_u, end_u, n, dtype=np.int64)).view('M8[ns]'), schema=[("datetime", pl.Datetime)])
  
  min_date=pd.to_datetime("2010-01-01")
  max_date=pd.to_datetime("2021-01-01")
  dfp = ppl(min_date, max_date, 1_000_000_000)
  dfp = dfp.sort(by="datetime")
  dfp = dfp.with_columns(value = pl.lit(np.random.rand(dfp.height)))
  
   dfp_period = dfp.with_columns(pl.col('datetime').dt.strftime('%Y_%m_%d').alias('period'))
   for name, dfpx in dfp_period.partition_by("period", as_dict=True).items():
     filename = f"C:/Users/lemton00/OneDrive - Sibelco/DEV/DATA/timeseries_part//{name}.parquet"
     dfpx.drop('period').write_parquet(filename)
  ```
  
  ```python
  import polars as pl
  df = pl.scan_parquet("C:/Users/lemton00/OneDrive - Sibelco/DEV/DATA/timeseries_part/*.parquet").select("datetime").max().collect()
  print(df)
  ```
- splitsing per dag: 4018 bestanden, 2198 kB per file, totaal 8.39 GB
	- Scan max datetime: 7 sec
- splitsing per jaar: 11 files, 780MB per file, totaal 8.39 GB
   Scan max datetime: 7 sec
- ### Generate random data 15 jaar minuut resolutie = 7_884_000
- splitsing per jaar: 11 files, 8.26MB per file, totaal 91 MB
   Scan max datetime: 100ms
- splitsing per maand: 132 files, 721 kB per file, totaal 91 MB
   Scan max datetime: 110ms
   filter data (6months): 140 ms
- splitsing per dag: 4018 bestanden, 25 kB per file, totaal 93 MB
   Scan max datetime: 10 ms
   filter data (6months): 140 ms